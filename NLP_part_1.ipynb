{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing text data\n",
    "\n",
    "### What tasks can be solved by processing text?\n",
    "\n",
    "\n",
    "syntactic tasks\n",
    "markup by parts of speech and morphological features\n",
    "division of words in the text into morphemes (suffix, prefix, etc.)\n",
    "stemming, lemmatization\n",
    "division into sentences (initials and abbreviations) and words (Chinese)\n",
    "search for names and titles in the text - entities (named entity recognition)\n",
    "resolution of the meaning of words in a given context (lock / lock)\n",
    "build syntax tree\n",
    "determining what other objects the word refers to\n",
    "text comprehension tasks that include a \"teacher\"\n",
    "next character prediction\n",
    "information search\n",
    "sentiment analysis\n",
    "highlighting relationships and facts\n",
    "answers on questions\n",
    "understanding and generating text\n",
    "text generation\n",
    "Machine translate\n",
    "dialogue models (chatbot)\n",
    "Indirect tasks:\n",
    "\n",
    "image description\n",
    "speech recognition\n",
    "Business objectives:\n",
    "\n",
    "speech recognition (assistant)\n",
    "chat bot (replacement of technical support in solving most of the issues)\n",
    "search for an exact answer to a question in a document base (for example, a base of standards)\n",
    "evaluation of the opinion in social networks about the product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download()  # download lots of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From text to simple models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting into tokens\n",
    "** Def. **\n",
    "splitting a sequence of characters into parts (tokens), possibly excluding some characters from consideration\n",
    "Naive approach: split the string with spaces and strip out punctuation marks\n",
    "\n",
    "\n",
    "* Tricia loved New York as loving New York could positively influence her career. *\n",
    "\n",
    "\n",
    "**Problems:**\n",
    "* my.email@mail.ru, 127.0.0.1\n",
    "* C ++, C #\n",
    "* York University vs New York University\n",
    "* Language dependence (“Lebensversicherungsgesellschaftsangestellter”, “l’amour”)\n",
    "Alternative: n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Трисия\n",
      "любила\n",
      "Нью\n",
      "-\n",
      "Йорк\n",
      ",\n",
      "поскольку\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer('\\w+|[^\\w\\s]+')\n",
    "s = u'Трисия любила Нью-Йорк, поскольку любовь к Нью-Йорку могла положительно повлиять на ее карьеру.'\n",
    "\n",
    "for t in tokenizer.tokenize(s)[:7]: \n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ftfy: fixes text for you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm blue, da ba dee da ba doo...\n"
     ]
    }
   ],
   "source": [
    "from ftfy import fix_text\n",
    "print(fix_text(u'\\001\\033[36;44mI&#x92;m blue, da ba dee da ba doo&#133;\\033[0m', normalization='NFKC'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop-Words\n",
    "\n",
    "The most frequent words in a language that do not contain any information about the content of the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "и в во не что он на я с со как а то все она так его но да ты\n",
      "i me my myself we our ours ourselves you you're you've you'll you'd your yours yourself yourselves he him his\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "print (' '.join(stopwords.words('russian')[:20]))\n",
    "print (' '.join(stopwords.words('english')[:20]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization\n",
    "\n",
    "Bringing tokens to a uniform look in order to get rid of superficial differences in spelling\n",
    "\n",
    "Approaches\n",
    "* formulate a set of rules according to which the token is converted\n",
    "New york → new york → new york → new york\n",
    "* explicitly store connections between tokens (WordNet - Princeton)\n",
    "machine -> car, Windows 6 -> window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "нью-йорк\n"
     ]
    }
   ],
   "source": [
    "s = 'Нью-Йорк'\n",
    "s1 = s.lower()\n",
    "print(s1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ньюйорк\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "s2 = re.sub(\"\\W\", \"\", s1)\n",
    "print(s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ньюиорк\n"
     ]
    }
   ],
   "source": [
    "s3 = re.sub(\"й\", u\"и\", s2)\n",
    "print(s3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming and Lemmatization\n",
    "\n",
    "Bringing the grammatical forms of a word and cognate words to a single stem (lemma):\n",
    "\n",
    "* Stemming - using simple heuristic rules\n",
    "   * Porter (Cambridge - 1980)\n",
    "         5 stages, each one applies a set of rules such as\n",
    "             sses → ss (caresses → caress)\n",
    "             ies → i (ponies → poni)\n",
    "\n",
    "   * Lovins (1968)\n",
    "   * Paice (1990)\n",
    "   * others\n",
    "* Lemmatization - using dictionaries and morphological analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token\n",
      "stem\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import PorterStemmer\n",
    "s = PorterStemmer()\n",
    "print (s.stem('Tokenization'))\n",
    "print (s.stem('stemming'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "авиац\n",
      "национальн\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import RussianStemmer\n",
    "r = RussianStemmer()\n",
    "print(r.stem('Авиация'))\n",
    "print(r.stem('национальный'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization\n",
    "(usually works better for complex languages, including Russian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata: Parse(word='замок', tag=OpencorporaTag('NOUN,inan,masc sing,nomn'), normal_form='замок', score=0.3333333333333333, methods_stack=((<DictionaryAnalyzer>, 'замок', 139, 0),))\n",
      "Word: замок | Normal form: замок\n",
      "\n",
      "\n",
      "Metadata: Parse(word='замок', tag=OpencorporaTag('NOUN,inan,masc sing,accs'), normal_form='замок', score=0.3333333333333333, methods_stack=((<DictionaryAnalyzer>, 'замок', 139, 3),))\n",
      "Word: замок | Normal form: замок\n",
      "\n",
      "\n",
      "Metadata: Parse(word='замок', tag=OpencorporaTag('VERB,perf,intr masc,sing,past,indc'), normal_form='замокнуть', score=0.3333333333333333, methods_stack=((<DictionaryAnalyzer>, 'замок', 730, 1),))\n",
      "Word: замок | Normal form: замокнуть\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pymorphy2\n",
    "\n",
    "import pymorphy2\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "for i in morph.parse(u'замок'):\n",
    "    print(\"Metadata: {}\".format(i))\n",
    "    print(\"Word: {} | Normal form: {}\".format(i.word, i.normal_form))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document submission\n",
    "\n",
    "** Boolean Model. ** The presence or absence of a word in the document\n",
    "** Bag of Words. ** Token order is not important\n",
    "\n",
    "* The weather was terrible, the princess was beautiful.\n",
    "Or was it the other way around? *\n",
    "\n",
    "Coordinates\n",
    "* Multinomial: the number of tokens in the document\n",
    "* Numeric: the weighted number of tokens in the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2., 0., 1.],\n",
       "       [0., 1., 3.]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dvectorizer = DictVectorizer(sparse=False)\n",
    "text_dict = [{'foo': 1, 'bar': 2}, {'foo': 3, 'baz': 1}]\n",
    "X = dvectorizer.fit_transform(text_dict)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'bar': 2.0, 'foo': 1.0}, {'baz': 1.0, 'foo': 3.0}]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dvectorizer.inverse_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 4.]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dvectorizer.transform({'foo': 4, 'unseen_feature': 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Counter({'thank': 1, '40': 1, ',': 1, 'mr': 1, 'president': 1, '.': 1}),\n",
       " Counter({'madam': 1,\n",
       "          'president': 1,\n",
       "          ',': 3,\n",
       "          'agree': 1,\n",
       "          'recognise': 1,\n",
       "          'turkey': 2,\n",
       "          \"'\": 1,\n",
       "          'european': 1,\n",
       "          'prospects': 2,\n",
       "          'auspicious': 1,\n",
       "          'outcome': 1,\n",
       "          'needs': 1,\n",
       "          ':': 1}),\n",
       " Counter({'madam': 1,\n",
       "          'president': 1,\n",
       "          ',': 2,\n",
       "          'firstly': 1,\n",
       "          'would': 1,\n",
       "          'like': 1,\n",
       "          'express': 1,\n",
       "          'sincerest': 1,\n",
       "          'thanks': 1,\n",
       "          'high': 1,\n",
       "          'representative': 1,\n",
       "          'including': 1,\n",
       "          'important': 1,\n",
       "          'issue': 1,\n",
       "          'agenda': 1,\n",
       "          'early': 1,\n",
       "          'stage': 1,\n",
       "          '.': 1})]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "docs = [\n",
    "    \"Thank 40 you, Mr President.\",\n",
    "    \"Madam President, I agree and recognise Turkey's European prospects, but if these prospects are to have an auspicious outcome, Turkey needs to:\",\n",
    "    \"Madam President, firstly, I would like to express my sincerest thanks to the High Representative for including this important issue in the agenda at such an early stage.\",\n",
    "]\n",
    "\n",
    "tokenizer = RegexpTokenizer('\\w+|[^\\w\\s]+')\n",
    "stopwords_eng = stopwords.words()\n",
    "\n",
    "document_bags = list()\n",
    "\n",
    "for d in docs:\n",
    "    bag = Counter()\n",
    "    text = d.lower()\n",
    "    for t in tokenizer.tokenize(text):     \n",
    "        if t in stopwords_eng:\n",
    "            continue\n",
    "        bag[t] += 1\n",
    "    document_bags.append(bag)\n",
    "    \n",
    "document_bags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [1., 3., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 1., 0., 1., 1., 1., 2., 1., 0., 0., 0., 0., 0., 2., 0.],\n",
       "       [0., 2., 1., 0., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 0., 0., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1.]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dvectorizer.fit_transform(document_bags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"'\",\n",
       " ',',\n",
       " '.',\n",
       " '40',\n",
       " ':',\n",
       " 'agenda',\n",
       " 'agree',\n",
       " 'auspicious',\n",
       " 'early',\n",
       " 'european',\n",
       " 'express',\n",
       " 'firstly',\n",
       " 'high',\n",
       " 'important',\n",
       " 'including',\n",
       " 'issue',\n",
       " 'like',\n",
       " 'madam',\n",
       " 'mr',\n",
       " 'needs',\n",
       " 'outcome',\n",
       " 'president',\n",
       " 'prospects',\n",
       " 'recognise',\n",
       " 'representative',\n",
       " 'sincerest',\n",
       " 'stage',\n",
       " 'thank',\n",
       " 'thanks',\n",
       " 'turkey',\n",
       " 'would']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dvectorizer.feature_names_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,\n",
       "         0, 0, 1, 0, 0],\n",
       "        [0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 2, 1, 0,\n",
       "         0, 0, 0, 0, 2],\n",
       "        [0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1,\n",
       "         1, 1, 0, 1, 0]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "sklearn_vectorizer = CountVectorizer(stop_words='english')\n",
    "sklearn_vectorizer.fit_transform(docs).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'thank': 23,\n",
       " '40': 0,\n",
       " 'mr': 14,\n",
       " 'president': 17,\n",
       " 'madam': 13,\n",
       " 'agree': 2,\n",
       " 'recognise': 19,\n",
       " 'turkey': 25,\n",
       " 'european': 5,\n",
       " 'prospects': 18,\n",
       " 'auspicious': 3,\n",
       " 'outcome': 16,\n",
       " 'needs': 15,\n",
       " 'firstly': 7,\n",
       " 'like': 12,\n",
       " 'express': 6,\n",
       " 'sincerest': 21,\n",
       " 'thanks': 24,\n",
       " 'high': 8,\n",
       " 'representative': 20,\n",
       " 'including': 10,\n",
       " 'important': 9,\n",
       " 'issue': 11,\n",
       " 'agenda': 1,\n",
       " 'early': 4,\n",
       " 'stage': 22}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "\n",
    "Количество вхождений слова $t$ в документе $d$\n",
    "$$\n",
    "TF_{t,d} = term\\!\\!-\\!\\!frequency(t, d)\n",
    "$$\n",
    "Количество документов из $N$ возможных, где встречается $t$\n",
    "$$\n",
    "DF_t = document\\!\\!-\\!\\!fequency(t)\n",
    "$$\n",
    "$$\n",
    "IDF_t = inverse\\!\\!-\\!\\!document\\!\\!-\\!\\!frequency(t) = \\log \\frac{N}{DF_t}\n",
    "$$\n",
    "TF-IDF\n",
    "$$\n",
    "TF\\!\\!-\\!\\!IDF_{t,d} = TF_{t,d} \\times IDF_t\n",
    "$$\n",
    "\n",
    "Оценивает важность слова в контексте документа, являющегося частью корпуса\n",
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0.54645401, 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.54645401,\n",
       "         0.        , 0.        , 0.32274454, 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.54645401, 0.        ,\n",
       "         0.        ],\n",
       "        [0.        , 0.        , 0.25882751, 0.25882751, 0.        ,\n",
       "         0.25882751, 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.19684499, 0.        ,\n",
       "         0.25882751, 0.25882751, 0.1528677 , 0.51765502, 0.25882751,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.51765502],\n",
       "        [0.        , 0.26795858, 0.        , 0.        , 0.26795858,\n",
       "         0.        , 0.26795858, 0.26795858, 0.26795858, 0.26795858,\n",
       "         0.26795858, 0.26795858, 0.26795858, 0.20378941, 0.        ,\n",
       "         0.        , 0.        , 0.15826066, 0.        , 0.        ,\n",
       "         0.26795858, 0.26795858, 0.26795858, 0.        , 0.26795858,\n",
       "         0.        ]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "features = vectorizer.fit_transform(docs).todense()\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'thank': 23,\n",
       " '40': 0,\n",
       " 'mr': 14,\n",
       " 'president': 17,\n",
       " 'madam': 13,\n",
       " 'agree': 2,\n",
       " 'recognise': 19,\n",
       " 'turkey': 25,\n",
       " 'european': 5,\n",
       " 'prospects': 18,\n",
       " 'auspicious': 3,\n",
       " 'outcome': 16,\n",
       " 'needs': 15,\n",
       " 'firstly': 7,\n",
       " 'like': 12,\n",
       " 'express': 6,\n",
       " 'sincerest': 21,\n",
       " 'thanks': 24,\n",
       " 'high': 8,\n",
       " 'representative': 20,\n",
       " 'including': 10,\n",
       " 'important': 9,\n",
       " 'issue': 11,\n",
       " 'agenda': 1,\n",
       " 'early': 4,\n",
       " 'stage': 22}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.vocabulary_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
